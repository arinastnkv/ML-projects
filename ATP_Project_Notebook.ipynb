{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "colab": {
      "name": "Copy of Copy of ATP Project Notebook.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vkle_s_CWu_x",
        "colab_type": "text"
      },
      "source": [
        "## ATP Men's Tennis Predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tWbrWs-aWu_4",
        "colab_type": "text"
      },
      "source": [
        "In this tennis project, I look at the data containing results for the menâ€™s ATP Tour tournaments as well as a range of ITF-level events (the Olympic Tennis Tournament and the Davis Cup). Those are the most prestgious tennis tournaments which the majority of top-tier athletes take part in.\n",
        "\n",
        "The goal is to learn to predict the outcome of tennis matches based on the available information about players' personal characteristics and previous matches stats. I will try to solve two types of problems: regression and binary classification problems.\n",
        "\n",
        "In regression problem, I aim to predict the difference in final score between the winner and the loser. In classification problem, I will simply try to answer which of the two player is more likely to win and what the odds are. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XNzf_51EWu_6",
        "colab_type": "text"
      },
      "source": [
        "Initially, there are 20 yearly datasets, from January 2000 to March 2019.\n",
        "\n",
        "Below is the meaning of possibly confusing columns of the dataset:\n",
        "\n",
        "* w/l_ace = absolute number of aces performed by the winner/loser\n",
        "* w/l_df = number of double faults performed by the winner/loser\n",
        "* w/l_svpt = total serve points performed by the winner/loser\n",
        "* w/l_1stin = number of 1st serves in performed by the winner/loser\n",
        "* w/l_1st won = points won on 1st serve by the winner/loser\n",
        "* w/l_2ndwon = points won on 2nd serve by the winner/loser\n",
        "* w/l_SvGms = serve games performed by the winner/loser\n",
        "* w/l_bpSaved = number of break points saved by the winner/loser\n",
        "* w/l_bpFaced = number of break points faced by the winner/loser\n",
        "* winner/loser_entry = indication of whether the winner/loser gained indirect acceptance (i.e. as a qualifier or wild card) into the main draw of a tournament\n",
        "* winner/loser_hand = winner's/loser's dominant hand\n",
        "* winner/loser_ht = winner's/loser's height\n",
        "* winner/loser_ioc = winner's/loser's home country"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l6jWUIqBWu_9",
        "colab_type": "text"
      },
      "source": [
        "#### Data cleaning and feature engineering "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2P0jtI8lWvAD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sklearn as sk\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import feature_selection\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d64V14qTW1k4",
        "colab_type": "code",
        "outputId": "ebd51a31-c103-48e0-a60a-69e3b0f7c11d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y4SbTqwjWvAO",
        "colab_type": "text"
      },
      "source": [
        "Importing the following dataframes and merging them into one dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "VssmiOSuWvAR",
        "colab_type": "code",
        "outputId": "72704c17-1582-4bf0-a3cd-16f05af7c416",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "import glob\n",
        "print('All csv files in data directory:', glob.glob('/content/gdrive/My Drive/Brainstation/ATP/atp_matches_*.csv'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "All csv files in data directory: ['/content/gdrive/My Drive/Brainstation/ATP/atp_matches_2019.csv', '/content/gdrive/My Drive/Brainstation/ATP/atp_matches_2010.csv', '/content/gdrive/My Drive/Brainstation/ATP/atp_matches_2009.csv', '/content/gdrive/My Drive/Brainstation/ATP/atp_matches_2018.csv', '/content/gdrive/My Drive/Brainstation/ATP/atp_matches_2008.csv', '/content/gdrive/My Drive/Brainstation/ATP/atp_matches_2004.csv', '/content/gdrive/My Drive/Brainstation/ATP/atp_matches_2005.csv', '/content/gdrive/My Drive/Brainstation/ATP/atp_matches_2007.csv', '/content/gdrive/My Drive/Brainstation/ATP/atp_matches_2011.csv', '/content/gdrive/My Drive/Brainstation/ATP/atp_matches_2013.csv', '/content/gdrive/My Drive/Brainstation/ATP/atp_matches_2012.csv', '/content/gdrive/My Drive/Brainstation/ATP/atp_matches_2006.csv', '/content/gdrive/My Drive/Brainstation/ATP/atp_matches_2002.csv', '/content/gdrive/My Drive/Brainstation/ATP/atp_matches_2003.csv', '/content/gdrive/My Drive/Brainstation/ATP/atp_matches_2016.csv', '/content/gdrive/My Drive/Brainstation/ATP/atp_matches_2017.csv', '/content/gdrive/My Drive/Brainstation/ATP/atp_matches_2000.csv', '/content/gdrive/My Drive/Brainstation/ATP/atp_matches_2001.csv', '/content/gdrive/My Drive/Brainstation/ATP/atp_matches_2014.csv', '/content/gdrive/My Drive/Brainstation/ATP/atp_matches_2015.csv']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9eU42mlHWvAY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "file_name = '/content/gdrive/My Drive/Brainstation/ATP/atp_matches_{}.csv'\n",
        "atp_l = []\n",
        "for i in range(2000, 2020):\n",
        "    atp_l.append(pd.read_csv(file_name.format(i)))\n",
        "atp = pd.concat(atp_l)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eWsysDzBWvAc",
        "colab_type": "text"
      },
      "source": [
        "Let's take a look at the fisrt five rows and the information about the dataset. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "pybenIs4WvAh",
        "colab_type": "code",
        "outputId": "e2fb3023-32f3-4009-b951-f4c58fac6566",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        }
      },
      "source": [
        "atp.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tourney_id</th>\n",
              "      <th>tourney_name</th>\n",
              "      <th>surface</th>\n",
              "      <th>draw_size</th>\n",
              "      <th>tourney_level</th>\n",
              "      <th>tourney_date</th>\n",
              "      <th>match_num</th>\n",
              "      <th>winner_id</th>\n",
              "      <th>winner_seed</th>\n",
              "      <th>winner_entry</th>\n",
              "      <th>winner_name</th>\n",
              "      <th>winner_hand</th>\n",
              "      <th>winner_ht</th>\n",
              "      <th>winner_ioc</th>\n",
              "      <th>winner_age</th>\n",
              "      <th>loser_id</th>\n",
              "      <th>loser_seed</th>\n",
              "      <th>loser_entry</th>\n",
              "      <th>loser_name</th>\n",
              "      <th>loser_hand</th>\n",
              "      <th>loser_ht</th>\n",
              "      <th>loser_ioc</th>\n",
              "      <th>loser_age</th>\n",
              "      <th>score</th>\n",
              "      <th>best_of</th>\n",
              "      <th>round</th>\n",
              "      <th>minutes</th>\n",
              "      <th>w_ace</th>\n",
              "      <th>w_df</th>\n",
              "      <th>w_svpt</th>\n",
              "      <th>w_1stIn</th>\n",
              "      <th>w_1stWon</th>\n",
              "      <th>w_2ndWon</th>\n",
              "      <th>w_SvGms</th>\n",
              "      <th>w_bpSaved</th>\n",
              "      <th>w_bpFaced</th>\n",
              "      <th>l_ace</th>\n",
              "      <th>l_df</th>\n",
              "      <th>l_svpt</th>\n",
              "      <th>l_1stIn</th>\n",
              "      <th>l_1stWon</th>\n",
              "      <th>l_2ndWon</th>\n",
              "      <th>l_SvGms</th>\n",
              "      <th>l_bpSaved</th>\n",
              "      <th>l_bpFaced</th>\n",
              "      <th>winner_rank</th>\n",
              "      <th>winner_rank_points</th>\n",
              "      <th>loser_rank</th>\n",
              "      <th>loser_rank_points</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2000-339</td>\n",
              "      <td>Adelaide</td>\n",
              "      <td>Hard</td>\n",
              "      <td>NaN</td>\n",
              "      <td>A</td>\n",
              "      <td>20000103</td>\n",
              "      <td>1</td>\n",
              "      <td>102358</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Thomas Enqvist</td>\n",
              "      <td>R</td>\n",
              "      <td>190.0</td>\n",
              "      <td>SWE</td>\n",
              "      <td>25.81</td>\n",
              "      <td>103096</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Arnaud Clement</td>\n",
              "      <td>R</td>\n",
              "      <td>173.0</td>\n",
              "      <td>FRA</td>\n",
              "      <td>22.05</td>\n",
              "      <td>6-3 6-4</td>\n",
              "      <td>3</td>\n",
              "      <td>R32</td>\n",
              "      <td>76.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>66.0</td>\n",
              "      <td>29.0</td>\n",
              "      <td>23.0</td>\n",
              "      <td>23.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>59.0</td>\n",
              "      <td>37.0</td>\n",
              "      <td>25.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>1850.0</td>\n",
              "      <td>56.0</td>\n",
              "      <td>490.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2000-339</td>\n",
              "      <td>Adelaide</td>\n",
              "      <td>Hard</td>\n",
              "      <td>NaN</td>\n",
              "      <td>A</td>\n",
              "      <td>20000103</td>\n",
              "      <td>2</td>\n",
              "      <td>103819</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Roger Federer</td>\n",
              "      <td>R</td>\n",
              "      <td>185.0</td>\n",
              "      <td>SUI</td>\n",
              "      <td>18.40</td>\n",
              "      <td>102533</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Jens Knippschild</td>\n",
              "      <td>R</td>\n",
              "      <td>190.0</td>\n",
              "      <td>GER</td>\n",
              "      <td>24.88</td>\n",
              "      <td>6-1 6-4</td>\n",
              "      <td>3</td>\n",
              "      <td>R32</td>\n",
              "      <td>45.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>46.0</td>\n",
              "      <td>28.0</td>\n",
              "      <td>24.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>42.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>64.0</td>\n",
              "      <td>515.0</td>\n",
              "      <td>91.0</td>\n",
              "      <td>404.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2000-339</td>\n",
              "      <td>Adelaide</td>\n",
              "      <td>Hard</td>\n",
              "      <td>NaN</td>\n",
              "      <td>A</td>\n",
              "      <td>20000103</td>\n",
              "      <td>3</td>\n",
              "      <td>102998</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Jan Michael Gambill</td>\n",
              "      <td>R</td>\n",
              "      <td>190.0</td>\n",
              "      <td>USA</td>\n",
              "      <td>22.58</td>\n",
              "      <td>101885</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Wayne Arthurs</td>\n",
              "      <td>L</td>\n",
              "      <td>190.0</td>\n",
              "      <td>AUS</td>\n",
              "      <td>28.80</td>\n",
              "      <td>3-6 7-6(5) 6-4</td>\n",
              "      <td>3</td>\n",
              "      <td>R32</td>\n",
              "      <td>115.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>81.0</td>\n",
              "      <td>40.0</td>\n",
              "      <td>35.0</td>\n",
              "      <td>28.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>26.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>103.0</td>\n",
              "      <td>59.0</td>\n",
              "      <td>49.0</td>\n",
              "      <td>22.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>58.0</td>\n",
              "      <td>544.0</td>\n",
              "      <td>105.0</td>\n",
              "      <td>243.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2000-339</td>\n",
              "      <td>Adelaide</td>\n",
              "      <td>Hard</td>\n",
              "      <td>NaN</td>\n",
              "      <td>A</td>\n",
              "      <td>20000103</td>\n",
              "      <td>4</td>\n",
              "      <td>103206</td>\n",
              "      <td>7.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Sebastien Grosjean</td>\n",
              "      <td>R</td>\n",
              "      <td>175.0</td>\n",
              "      <td>FRA</td>\n",
              "      <td>21.60</td>\n",
              "      <td>102776</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Andrew Ilie</td>\n",
              "      <td>R</td>\n",
              "      <td>180.0</td>\n",
              "      <td>AUS</td>\n",
              "      <td>23.71</td>\n",
              "      <td>6-2 6-1</td>\n",
              "      <td>3</td>\n",
              "      <td>R32</td>\n",
              "      <td>65.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>66.0</td>\n",
              "      <td>35.0</td>\n",
              "      <td>28.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>49.0</td>\n",
              "      <td>22.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>27.0</td>\n",
              "      <td>928.0</td>\n",
              "      <td>54.0</td>\n",
              "      <td>602.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2000-339</td>\n",
              "      <td>Adelaide</td>\n",
              "      <td>Hard</td>\n",
              "      <td>NaN</td>\n",
              "      <td>A</td>\n",
              "      <td>20000103</td>\n",
              "      <td>5</td>\n",
              "      <td>102796</td>\n",
              "      <td>3.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Magnus Norman</td>\n",
              "      <td>R</td>\n",
              "      <td>188.0</td>\n",
              "      <td>SWE</td>\n",
              "      <td>23.59</td>\n",
              "      <td>102401</td>\n",
              "      <td>NaN</td>\n",
              "      <td>WC</td>\n",
              "      <td>Scott Draper</td>\n",
              "      <td>L</td>\n",
              "      <td>178.0</td>\n",
              "      <td>AUS</td>\n",
              "      <td>25.58</td>\n",
              "      <td>6-4 6-4</td>\n",
              "      <td>3</td>\n",
              "      <td>R32</td>\n",
              "      <td>68.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>52.0</td>\n",
              "      <td>32.0</td>\n",
              "      <td>26.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>73.0</td>\n",
              "      <td>40.0</td>\n",
              "      <td>25.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>1244.0</td>\n",
              "      <td>154.0</td>\n",
              "      <td>219.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  tourney_id tourney_name  ... loser_rank  loser_rank_points\n",
              "0   2000-339     Adelaide  ...       56.0              490.0\n",
              "1   2000-339     Adelaide  ...       91.0              404.0\n",
              "2   2000-339     Adelaide  ...      105.0              243.0\n",
              "3   2000-339     Adelaide  ...       54.0              602.0\n",
              "4   2000-339     Adelaide  ...      154.0              219.0\n",
              "\n",
              "[5 rows x 49 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5WSnZoYJWvAp",
        "colab_type": "code",
        "outputId": "777952e8-b384-4b79-c58f-359323be1a7e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 935
        }
      },
      "source": [
        "atp.info()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 59430 entries, 0 to 650\n",
            "Data columns (total 49 columns):\n",
            "tourney_id            59430 non-null object\n",
            "tourney_name          59430 non-null object\n",
            "surface               59312 non-null object\n",
            "draw_size             0 non-null float64\n",
            "tourney_level         59430 non-null object\n",
            "tourney_date          59430 non-null int64\n",
            "match_num             59430 non-null int64\n",
            "winner_id             59430 non-null int64\n",
            "winner_seed           24690 non-null float64\n",
            "winner_entry          7048 non-null object\n",
            "winner_name           59430 non-null object\n",
            "winner_hand           59412 non-null object\n",
            "winner_ht             55024 non-null float64\n",
            "winner_ioc            59430 non-null object\n",
            "winner_age            59416 non-null float64\n",
            "loser_id              59430 non-null int64\n",
            "loser_seed            13408 non-null float64\n",
            "loser_entry           11686 non-null object\n",
            "loser_name            59430 non-null object\n",
            "loser_hand            59387 non-null object\n",
            "loser_ht              52294 non-null float64\n",
            "loser_ioc             59430 non-null object\n",
            "loser_age             59400 non-null float64\n",
            "score                 59429 non-null object\n",
            "best_of               59430 non-null int64\n",
            "round                 59430 non-null object\n",
            "minutes               52431 non-null float64\n",
            "w_ace                 53749 non-null float64\n",
            "w_df                  53749 non-null float64\n",
            "w_svpt                53749 non-null float64\n",
            "w_1stIn               53749 non-null float64\n",
            "w_1stWon              53749 non-null float64\n",
            "w_2ndWon              53749 non-null float64\n",
            "w_SvGms               53749 non-null float64\n",
            "w_bpSaved             53749 non-null float64\n",
            "w_bpFaced             53749 non-null float64\n",
            "l_ace                 53749 non-null float64\n",
            "l_df                  53749 non-null float64\n",
            "l_svpt                53749 non-null float64\n",
            "l_1stIn               53749 non-null float64\n",
            "l_1stWon              53749 non-null float64\n",
            "l_2ndWon              53749 non-null float64\n",
            "l_SvGms               53749 non-null float64\n",
            "l_bpSaved             53749 non-null float64\n",
            "l_bpFaced             53749 non-null float64\n",
            "winner_rank           58932 non-null float64\n",
            "winner_rank_points    58932 non-null float64\n",
            "loser_rank            58161 non-null float64\n",
            "loser_rank_points     58161 non-null float64\n",
            "dtypes: float64(30), int64(5), object(14)\n",
            "memory usage: 22.7+ MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pjm1InLuWvAt",
        "colab_type": "text"
      },
      "source": [
        "To begin with, I drop several columns which will not be useful in our case, such as tournament ID, the number of match, player's seed (which is obviously correlated with rank), etc. I also drop draw size column which is empty. We can also notice that out of 59,430 observations the majority of variables do not have as many NaN values. The only columns which are lacking observations are 'winner/loser seed' (which will be dropped) and 'winner/loser entry'. The latter one contains quite important information about the players, therefore it will be necessary to transform the variable in order to reduce the number of missing values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mj61uM0LWvAt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "atp = atp.drop(['draw_size', 'tourney_id', 'match_num', 'winner_seed', 'loser_seed', 'best_of'], axis = 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fWa0dBzLWvAw",
        "colab_type": "text"
      },
      "source": [
        "Next I check if there are any duplicates - turns out there are none:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZJsztH5QWvAx",
        "colab_type": "code",
        "outputId": "12d17242-5850-45dc-8d34-652eefdd2353",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "atp[atp.duplicated()]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tourney_name</th>\n",
              "      <th>surface</th>\n",
              "      <th>tourney_level</th>\n",
              "      <th>tourney_date</th>\n",
              "      <th>winner_id</th>\n",
              "      <th>winner_entry</th>\n",
              "      <th>winner_name</th>\n",
              "      <th>winner_hand</th>\n",
              "      <th>winner_ht</th>\n",
              "      <th>winner_ioc</th>\n",
              "      <th>winner_age</th>\n",
              "      <th>loser_id</th>\n",
              "      <th>loser_entry</th>\n",
              "      <th>loser_name</th>\n",
              "      <th>loser_hand</th>\n",
              "      <th>loser_ht</th>\n",
              "      <th>loser_ioc</th>\n",
              "      <th>loser_age</th>\n",
              "      <th>score</th>\n",
              "      <th>round</th>\n",
              "      <th>minutes</th>\n",
              "      <th>w_ace</th>\n",
              "      <th>w_df</th>\n",
              "      <th>w_svpt</th>\n",
              "      <th>w_1stIn</th>\n",
              "      <th>w_1stWon</th>\n",
              "      <th>w_2ndWon</th>\n",
              "      <th>w_SvGms</th>\n",
              "      <th>w_bpSaved</th>\n",
              "      <th>w_bpFaced</th>\n",
              "      <th>l_ace</th>\n",
              "      <th>l_df</th>\n",
              "      <th>l_svpt</th>\n",
              "      <th>l_1stIn</th>\n",
              "      <th>l_1stWon</th>\n",
              "      <th>l_2ndWon</th>\n",
              "      <th>l_SvGms</th>\n",
              "      <th>l_bpSaved</th>\n",
              "      <th>l_bpFaced</th>\n",
              "      <th>winner_rank</th>\n",
              "      <th>winner_rank_points</th>\n",
              "      <th>loser_rank</th>\n",
              "      <th>loser_rank_points</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [tourney_name, surface, tourney_level, tourney_date, winner_id, winner_entry, winner_name, winner_hand, winner_ht, winner_ioc, winner_age, loser_id, loser_entry, loser_name, loser_hand, loser_ht, loser_ioc, loser_age, score, round, minutes, w_ace, w_df, w_svpt, w_1stIn, w_1stWon, w_2ndWon, w_SvGms, w_bpSaved, w_bpFaced, l_ace, l_df, l_svpt, l_1stIn, l_1stWon, l_2ndWon, l_SvGms, l_bpSaved, l_bpFaced, winner_rank, winner_rank_points, loser_rank, loser_rank_points]\n",
              "Index: []"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TU3h-dR0WvA0",
        "colab_type": "text"
      },
      "source": [
        "To check the data for high cardinality, it is necessary to investigate columns and find out whether any of categorical variables has a large number of unique values:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a49VZgXrWvA2",
        "colab_type": "code",
        "outputId": "14975871-b306-4018-9260-273322311e4a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "atp.select_dtypes(include=[object]).nunique()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tourney_name      1582\n",
              "surface              5\n",
              "tourney_level        5\n",
              "winner_entry         6\n",
              "winner_name       1457\n",
              "winner_hand          3\n",
              "winner_ioc         104\n",
              "loser_entry          7\n",
              "loser_name        2160\n",
              "loser_hand           3\n",
              "loser_ioc          111\n",
              "score            10756\n",
              "round                9\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p_U1GZPtWvA5",
        "colab_type": "text"
      },
      "source": [
        "Apparently, tourney/winner/loser name columns have a high number of unique values - more than one thousand. That's not unexpected, but that's not a problem as these variables won't be used in the analysis. However, there are two variables, both standing for players' nationality, which might be useful. To get rid of a huge number of categories, those will be transformed later. The score column will be transormed as well into a target variable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mSHm6H8uWvA7",
        "colab_type": "text"
      },
      "source": [
        "Then it is necessary to convert categorical data to numerical. For 'surface', 1 means the most used surface while 4 means the least used surface according to the frequency below. None values will be dropped as there are only 80 of them:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "eP--OkkTWvA9",
        "colab_type": "code",
        "outputId": "cc9c3694-33c0-47ba-9c3b-8956a00d527c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "atp['surface'].value_counts()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Hard      31675\n",
              "Clay      19449\n",
              "Grass      6084\n",
              "Carpet     2024\n",
              "None         80\n",
              "Name: surface, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 154
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yXgoMBlMWvBA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "atp = atp[atp['surface'] != 'None']\n",
        "atp['surface'] = atp['surface'].map({'Hard' : 1, 'Clay': 2, 'Grass': 3, 'Carpet': 4})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T_0CsAkJWvBC",
        "colab_type": "text"
      },
      "source": [
        "Same approach is used for player's dominant hand except for the fact that right hand will be a default characteristic and marked as zero. The tournaments' level variable is converted according to the ranking of the tournaments with Grand Slams as the most prestigious ones."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dvHtKcAWWvBC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#R - right-handed, L - left-handed, U - ambidextrous\n",
        "atp['winner_hand'] = atp['winner_hand'].map({'R' : 0, 'L' : 1, 'U' : 2})\n",
        "atp['loser_hand'] = atp['loser_hand'].map({'R' : 0, 'L' : 1, 'U' : 2})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zuT8b0ISWvBE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#A - ATP250 & ATP500, M - ATP Masters 1000, G - Grand Slam, D - Davis Cup, F - Finals & Finals Next Gen\n",
        "atp['tourney_level'] = atp['tourney_level'].map({'G': 1, 'F': 2, 'M': 3, 'A': 4, 'D': 5})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "scrolled": false,
        "id": "Ft4i9sHeWvBG",
        "colab_type": "text"
      },
      "source": [
        "It was mentioned that 'winner/loser entry' columns contain a lot of null values which is expected since the majority of the players get to the main draw by their rank, and there are not so many exceptions such as wild cards or qualifiers. However, such information is still valuable, and I decided to create new binary columns; each would indicate particular special conditions under which previously unqualified players got a chance to play in the main draw:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W63FEZiiWvBH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Q - qualifier; WC - wild card; LL - lucky loser, ALT - alternate\n",
        "#PR - protected ranking, SE - special extempt\n",
        "for i in ['winner', 'loser']:\n",
        "    atp['{}_Q'.format(i)] = np.where(atp['{}_entry'.format(i)] == 'Q', 1, 0)\n",
        "    atp['{}_WC'.format(i)] = np.where(((atp['{}_entry'.format(i)] == 'WC') | (atp['{}_entry'.format(i)] == 'SE')), 1, 0)\n",
        "    atp['{}_LL'.format(i)] = np.where(((atp['{}_entry'.format(i)] == 'LL') | (atp['{}_entry'.format(i)] == 'ALT')), 1, 0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ePYfuAa4AeA",
        "colab_type": "text"
      },
      "source": [
        "Now it's also necessary to start preparing separate dataset for the classification problem. Since now there are columns for winners and losers, we need to shuffle the observations to be able to create binary dependent variable. To make the dataset balanced, I create 45% of 0's and 65% of 1's:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RF4ONjacKCSY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df1, df2 = train_test_split(atp, test_size=0.45)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WOVJntfOKIpT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df1.columns = ['tourney_name', 'surface', 'tourney_level', 'tourney_date', 'p1_id',\n",
        "       'p1_entry', 'p1_name', 'p1_hand', 'p1_ht', 'p1_ioc',\n",
        "       'p1_age', 'p2_id', 'p2_entry', 'p2_name', 'p2_hand',\n",
        "       'p2_ht', 'p2_ioc', 'p2_age', 'score', 'round', 'minutes',\n",
        "       'p1_ace', 'p1_df', 'p1_svpt', 'p1_1stIn', 'p1_1stWon', 'p1_2ndWon', 'p1_SvGms',\n",
        "       'p1_bpSaved', 'p1_bpFaced', 'p2_ace', 'p2_df', 'p2_svpt', 'p2_1stIn',\n",
        "       'p2_1stWon', 'p2_2ndWon', 'p2_SvGms', 'p2_bpSaved', 'p2_bpFaced',\n",
        "       'p1_rank', 'p1_rank_points', 'p2_rank', 'p2_rank_points', 'p1_Q', 'p1_WC', 'p1_LL', 'p2_Q', 'p2_WC',\n",
        "       'p2_LL']\n",
        "df1['outcome'] = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ygLv_0O1KdN3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df2.columns = ['tourney_name', 'surface', 'tourney_level', 'tourney_date', 'p2_id',\n",
        "       'p2_entry', 'p2_name', 'p2_hand', 'p2_ht', 'p2_ioc',\n",
        "       'p2_age', 'p1_id', 'p1_entry', 'p1_name', 'p1_hand',\n",
        "       'p1_ht', 'p1_ioc', 'p1_age', 'score', 'round', 'minutes',\n",
        "       'p2_ace', 'p2_df', 'p2_svpt', 'p2_1stIn', 'p2_1stWon', 'p2_2ndWon', 'p2_SvGms',\n",
        "       'p2_bpSaved', 'p2_bpFaced', 'p1_ace', 'p1_df', 'p1_svpt', 'p1_1stIn',\n",
        "       'p1_1stWon', 'p1_2ndWon', 'p1_SvGms', 'p1_bpSaved', 'p1_bpFaced',\n",
        "       'p2_rank', 'p2_rank_points', 'p1_rank', 'p1_rank_points', 'p2_Q', 'p2_WC', 'p2_LL', 'p1_Q', 'p1_WC',\n",
        "       'p1_LL']\n",
        "df2['outcome'] = 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ho_m2eaSKoVo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "atp_class = pd.concat([df1, df2])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQtlwcz-WvBI",
        "colab_type": "text"
      },
      "source": [
        "While age and height are extremely important, it is worth introducing new variables describing the difference in height/age between the two players. In my opinion, those have more explanationary power and will help to get better predictions:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ataslR8uWvBJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "atp['ht_diff'] = atp['winner_ht'] - atp['loser_ht']\n",
        "atp['age_diff'] = atp['winner_age'] - atp['loser_age']\n",
        "\n",
        "atp_class['ht_diff'] = atp_class['p1_ht'] - atp_class['p2_ht']\n",
        "atp_class['age_diff'] = atp_class['p1_age'] - atp_class['p2_age']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZnPZagXOWvBL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "atp = atp.dropna(subset=['ht_diff', 'winner_hand', 'loser_hand'])\n",
        "\n",
        "atp_class = atp_class.dropna(subset=['ht_diff', 'p1_hand', 'p2_hand'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZ8MBA71WvBM",
        "colab_type": "text"
      },
      "source": [
        "At the beginning I found out that there are two categorical variables with many categories which can be used further. However, it is not that important to know precisely about a player's nationality. What can be informative is whether a player is from a leading tennis nation (i.e. which has huge internal competition, tennis traditions, tennis celebrities). I created a binary varible indicating that. The player is assigned with a value 1 if he is from any of 15 countries with the biggest number of representatives in the ATP tour, and 0 otherwise:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kLGyBT77WvBN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "big_countries = ['ESP', 'FRA', 'USA', 'ARG', 'GER', 'RUS', 'SUI', 'CZE', 'CRO', \n",
        "                 'AUS', 'SRB', 'ITA', 'SWE', 'GBR', 'BEL']\n",
        "for i in ['winner', 'loser']:\n",
        "    atp['{}_sign_country'.format(i)] = np.where(atp['{}_ioc'.format(i)].isin(big_countries), 1, 0)\n",
        "\n",
        "for i in ['p1', 'p2']:\n",
        "    atp_class['{}_sign_country'.format(i)] = np.where(atp_class['{}_ioc'.format(i)].isin(big_countries), 1, 0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fGkAtxBZWvBP",
        "colab_type": "text"
      },
      "source": [
        "Below is the format of the score variable:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h2l1em49WvBP",
        "colab_type": "code",
        "outputId": "ff6e5cfe-7eb0-40df-bfb9-ab98d470d8bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "atp['score'].head(3)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0           6-3 6-4\n",
              "1           6-1 6-4\n",
              "2    3-6 7-6(5) 6-4\n",
              "Name: score, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 170
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JcqtlpUrWvBS",
        "colab_type": "text"
      },
      "source": [
        "Of course, we can't use it to predict the outcome of the match since it's string. Therefore, the scores have been split by sets; NaN values were dropped. The incomplete matches (for example, match could end due to injury of one player) were also dropped. I also got rid of inconsistencies (such as 0-0 in the first/second set which can't be true).  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ZdHc6YsWvBS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "atp[['set1', 'set2', 'set3', 'set4', 'set5', 'set6']] = atp.score.str.split(\" \", expand=True,)\n",
        "\n",
        "atp = atp[(atp['set1'] != 'RET') & (atp['set1'] != 'DEF') & (atp['set1'] != '1-Apr') & (atp['set1'] != '3-Jun') & (atp['set1'] != 'W/O') & (atp['set1'] != 'Default')]\n",
        "atp = atp[(atp['set2'] != 'RET') & (atp['set2'] != 'DEF') & (atp['set2'] != 'W/O') & (atp['set2'] != 'Default')]\n",
        "atp = atp[(atp['set3'] != 'RET') & (atp['set3'] != 'Played') & (atp['set3'] != 'DEF') & (atp['set3'] != 'Default')]\n",
        "atp = atp[(atp['set4'] != 'RET') & (atp['set4'] != 'and') & (atp['set4'] != 'DEF') & (atp['set4'] != 'Default')]\n",
        "atp = atp[(atp['set5'] != 'RET') & (atp['set5'] != 'unfinished') & (atp['set5'] != 'DEF') & (atp['set5'] != 'Default')]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gfAkuAwGWvBU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(1,6):\n",
        "    atp['set{}'.format(i)] = np.where(((atp['set{}'.format(i)] == '') | (atp['set{}'.format(i)].isna())), '0-0', atp['set{}'.format(i)])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "umenymTrWvBV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "atp = atp[(atp['set1'] != '0-0') & (atp['set2'] != '0-0')]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "hKd3pEPFWvBX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range (1,6):\n",
        "    atp[['set{}_w'.format(i), 'set{}_l'.format(i)]] = atp['set{}'.format(i)].str.split(\"-\", expand=True,)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q2mjELqkWvBZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "atp = atp[(atp['set3_w'] != '[11') & (atp['set3_w'] != '[12')]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bicXzExiWvBb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in [1, 2, 4]:\n",
        "    atp['set{}_l'.format(i)] = atp['set{}_l'.format(i)].astype(str).str[:1].astype(int)\n",
        "    \n",
        "for j in [3, 5]:\n",
        "    atp['set{}_l'.format(j)] = atp['set{}_l'.format(j)].astype(str).str[:2].str.replace('(','').astype(int)\n",
        "    \n",
        "for k in range(1, 6):\n",
        "    atp['set{}_w'.format(k)] = atp['set{}_w'.format(k)].astype(int)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t-WdQgK2WvBc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "atp = atp.drop(['round', 'winner_rank_points', 'loser_rank_points', 'winner_ioc', 'loser_ioc', 'winner_entry', \n",
        "                'loser_entry','set1', 'set2', 'set3', 'set4', 'set5', 'set6'], axis = 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k09XkPfdWvBd",
        "colab_type": "text"
      },
      "source": [
        "Using the absolute numbers of matches' stats (the number of aces, saved breakpoints, successful serves, etc), we can calculate the percentages. Also, the total number of points won by the winner and the loser were calculated."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qeTTzs7gWvBe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in ['w', 'l']:\n",
        "    atp['total_{}_pts'.format(i)] = atp['set1_{}'.format(i)] + atp['set2_{}'.format(i)] + atp['set3_{}'.format(i)] + atp['set4_{}'.format(i)] + atp['set5_{}'.format(i)]\n",
        "    atp['{}_bpsaved_percentage'.format(i)] = np.where(atp['{}_bpFaced'.format(i)] != 0, round((100*atp['{}_bpSaved'.format(i)])/atp['{}_bpFaced'.format(i)], 2), 0)\n",
        "    atp['{}_1stIn_percentage'.format(i)] = round((100*atp['{}_1stIn'.format(i)])/atp['{}_svpt'.format(i)], 2)\n",
        "    atp['{}_1stWon_percentage'.format(i)] = round((100*atp['{}_1stWon'.format(i)])/atp['{}_svpt'.format(i)], 2)\n",
        "    atp['{}_2ndWon_percentage'.format(i)] = round((100*atp['{}_2ndWon'.format(i)])/atp['{}_svpt'.format(i)], 2)\n",
        "\n",
        "for i in ['p1', 'p2']:\n",
        "    atp_class['{}_bpsaved_percentage'.format(i)] = np.where(atp_class['{}_bpFaced'.format(i)] != 0, round((100*atp_class['{}_bpSaved'.format(i)])/atp_class['{}_bpFaced'.format(i)], 2), 0)\n",
        "    atp_class['{}_1stIn_percentage'.format(i)] = round((100*atp_class['{}_1stIn'.format(i)])/atp_class['{}_svpt'.format(i)], 2)\n",
        "    atp_class['{}_1stWon_percentage'.format(i)] = round((100*atp_class['{}_1stWon'.format(i)])/atp_class['{}_svpt'.format(i)], 2)\n",
        "    atp_class['{}_2ndWon_percentage'.format(i)] = round((100*atp_class['{}_2ndWon'.format(i)])/atp_class['{}_svpt'.format(i)], 2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q30RBnhwWvBf",
        "colab_type": "text"
      },
      "source": [
        "Finally, the target variable was created for the first dataset - score difference. Also, differences in key match stats were calculated - those are expected to be one of the most importants features in the analysis."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ur1m6TjlWvBg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "atp['pts_diff'] = atp['total_w_pts'] - atp['total_l_pts'] #target variable\n",
        "atp['bpsaved_diff'] = atp['w_bpsaved_percentage'] - atp['l_bpsaved_percentage']\n",
        "atp['aces_diff'] = atp['w_ace'] - atp['l_ace']\n",
        "atp['df_diff'] = atp['w_df'] - atp['l_df']\n",
        "atp['1stIn_diff'] = atp['w_1stIn_percentage'] - atp['l_1stIn_percentage']\n",
        "atp['1stWon_diff'] = atp['w_1stWon_percentage'] - atp['l_1stWon_percentage']\n",
        "atp['2ndWon_diff'] = atp['w_2ndWon_percentage'] - atp['l_2ndWon_percentage']\n",
        "\n",
        "atp_class['bpsaved_diff'] = atp_class['p1_bpsaved_percentage'] - atp_class['p2_bpsaved_percentage']\n",
        "atp_class['aces_diff'] = atp_class['p1_ace'] - atp_class['p2_ace']\n",
        "atp_class['df_diff'] = atp_class['p1_df'] - atp_class['p2_df']\n",
        "atp_class['1stIn_diff'] = atp_class['p1_1stIn_percentage'] - atp_class['p2_1stIn_percentage']\n",
        "atp_class['1stWon_diff'] = atp_class['p1_1stWon_percentage'] - atp_class['p2_1stWon_percentage']\n",
        "atp_class['2ndWon_diff'] = atp_class['p1_2ndWon_percentage'] - atp_class['p2_2ndWon_percentage']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7VcYJwigWvBh",
        "colab_type": "text"
      },
      "source": [
        "Finally, all other NaN values were dropped, and we're left with roughly 45 thousand observations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HMDI7RBDWvBi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "atp.dropna(inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-SFQTQvpfPZo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "atp_class = atp_class.drop(['round', 'p1_rank_points', 'p2_rank_points', 'p1_ioc', 'p2_ioc', 'p1_entry', \n",
        "                'p2_entry','score'], axis = 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uNMFbtUdM-pY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "atp_class.dropna(inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2g33mPHyWvBj",
        "colab_type": "text"
      },
      "source": [
        "#### Regression Problem"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dtAei7-VWvBk",
        "colab_type": "text"
      },
      "source": [
        "Below are the features which might be used in the subsequent analysis. Those are the variables which can greatly affect the outcome of a tennis match. Apparently, previous matches' stats can be used to determine whether any given player has chances to win based on his past performance. Personal characteristics such as age and height are important as well: younger and taller players might have an advantage, though there are some exceptions (i.e. Federer). Surface varible is extremely informative since players generally can't show the same quality of play on all surfaces. Those who hold wild cards or got into the main draw as lucky losers also have lower chances of winning against higher-ranked competitors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F8y7xV6_WvBk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "features = ['surface', 'tourney_level', \"winner_hand\", \"loser_hand\", \"minutes\", \"winner_rank\", \"loser_rank\",\n",
        "           'winner_Q', 'winner_WC', 'winner_LL', 'loser_Q', 'loser_WC', 'loser_LL', 'ht_diff', 'winner_sign_country',\n",
        "           'loser_sign_country', 'age_diff', 'bpsaved_diff', 'aces_diff', 'df_diff', '1stIn_diff', '1stWon_diff',\n",
        "           '2ndWon_diff', 'pts_diff']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QFh14UVKWvBl",
        "colab_type": "text"
      },
      "source": [
        "My dependent (target) variable is difference in final score between the first and the second player (i.e. winner and loser). Independent variables are listed above. Since the target variable is continuous, we need to apply regression methods. As a first step, I will perform multiple linear regression using all the variables. In general, you do not need to center or standardize your data for multiple regression, so I will not scale my data before performing this method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YFyPVaThWvBm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = atp[features].drop(['pts_diff'], axis = 1)\n",
        "y = atp['pts_diff']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yWwkVlTLWvBn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u90udr8VWvBo",
        "colab_type": "code",
        "outputId": "c72e1b56-4f28-43c3-8895-5c5bb7f87946",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "LinReg = LinearRegression()\n",
        "LinReg.fit(X_train, y_train)\n",
        "print(\"The coefficient of determination R^2 for train data is\", round(LinReg.score(X_train, y_train), 4))\n",
        "print(\"The coefficient of determination R^2 for test data is\", round(LinReg.score(X_test, y_test), 4))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The coefficient of determination R^2 for train data is 0.7016\n",
            "The coefficient of determination R^2 for test data is 0.7031\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QmTS1om2WvBr",
        "colab_type": "text"
      },
      "source": [
        "First of all, we can notice that the model is not overfitted since $R^2$ for test data does not differ from the coefficient of $R^2$ for train data. In general, all chosen explanatory variables give us the score of 70%, which is not bad for the simplest regression method."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_3rdK43uWvBs",
        "colab_type": "text"
      },
      "source": [
        "Of course, the number of features is too big right now (23). It is highly likely that many of them are not as important and we can probably reduce the number of variables. To do so, I perform the feature selection method which evaluates feature importances and selects the most relevant features:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_17wBypTWvBs",
        "colab_type": "code",
        "outputId": "ead07d34-9c6a-469f-b965-8116c45e4c4e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        }
      },
      "source": [
        "feature_rank = feature_selection.mutual_info_classif(X, y)\n",
        "feature_rank_df = pd.DataFrame(list(zip(features, feature_rank)), columns=['Feature', 'Score'])\n",
        "feature_rank_df.sort_values(by='Score', ascending = False).head(10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Feature</th>\n",
              "      <th>Score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>bpsaved_diff</td>\n",
              "      <td>0.287546</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>minutes</td>\n",
              "      <td>0.246105</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>1stWon_diff</td>\n",
              "      <td>0.144900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>tourney_level</td>\n",
              "      <td>0.059325</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>2ndWon_diff</td>\n",
              "      <td>0.047616</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>aces_diff</td>\n",
              "      <td>0.031483</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>winner_rank</td>\n",
              "      <td>0.018387</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>df_diff</td>\n",
              "      <td>0.017936</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>loser_rank</td>\n",
              "      <td>0.011447</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>surface</td>\n",
              "      <td>0.008655</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          Feature     Score\n",
              "17   bpsaved_diff  0.287546\n",
              "4         minutes  0.246105\n",
              "21    1stWon_diff  0.144900\n",
              "1   tourney_level  0.059325\n",
              "22    2ndWon_diff  0.047616\n",
              "18      aces_diff  0.031483\n",
              "5     winner_rank  0.018387\n",
              "19        df_diff  0.017936\n",
              "6      loser_rank  0.011447\n",
              "0         surface  0.008655"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kgpqkMkAWvBu",
        "colab_type": "text"
      },
      "source": [
        "We can see that the majority of features can be eliminated since they are not as relevant. Fisrt, I put the threshold of 0.05 and performed multiple linear regression again, but $R^2$ dropped significantly. That's why I decided to stick to the threshold of 0.01:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vjuPRHriWvBu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "selected = [x for x in feature_rank_df[feature_rank_df['Score'] >= 0.01]['Feature']]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QcSYmtrIWvBv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X[selected], y, test_size=0.3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-_MGA851WvBw",
        "colab_type": "code",
        "outputId": "3261a9cd-1322-4ec7-a842-6ce2b5c0ae86",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "LinReg2 = LinearRegression()\n",
        "LinReg2.fit(X_train, y_train)\n",
        "print(\"The coefficient of determination R^2 for train data is\", round(LinReg2.score(X_train, y_train), 4))\n",
        "print(\"The coefficient of determination R^2 for test data is\", round(LinReg2.score(X_test, y_test), 4))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The coefficient of determination R^2 for train data is 0.6984\n",
            "The coefficient of determination R^2 for test data is 0.7065\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9gEuWsorWvBy",
        "colab_type": "text"
      },
      "source": [
        "Now we can see that even though we used only 8 out of 23 initial features, $R^2$ for test data is even just a tiny bit higher. Thus, I will continue to use these selected features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AoFIQH-DWvBy",
        "colab_type": "text"
      },
      "source": [
        "Although there is no overfitting according to $R^2$, it would also be helpful to check whether there is collinearity, which would make estimators inconsistent. To check that, I'll take a look at the correlation between selected dependent variables:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "2Me5F4qUWvB0",
        "colab_type": "code",
        "outputId": "e3f0d21c-da6d-40c3-c1cf-850b614658ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 429
        }
      },
      "source": [
        "fig = plt.figure(figsize=(7, 7));\n",
        "sns.heatmap(X[selected].corr(), annot = True, cmap='coolwarm');\n",
        "fig.autofmt_xdate()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAdsAAAGcCAYAAACRJwrgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nI2a2MWLEiBEjRpbx/4VzjyY7z45fAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 504x504 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DMHXf95_WvB1",
        "colab_type": "text"
      },
      "source": [
        "The highest coefficient of correlation (in absolute value) is between the tourney level and the length of the matches. However, it does not even exceed -0.5 which means that there should not be multicollinearity. Therefore, there is no need in performing either Ridge or Lasso Regression."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LCZCLRU4WvB2",
        "colab_type": "text"
      },
      "source": [
        "While linear regression is one of the most common methods, it is not as flexible because it can't follow non-linear dependencies. If we use more complex methods, there is a high chance we'll be able to identify trends and patterns better."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-7PaRk1GWvB2",
        "colab_type": "text"
      },
      "source": [
        "To begin with, SVM method will be performed. Although it is usually used for classification problems, SVM can also be used as a regression method; it is called the Support Vector Regression, and it uses the same principles as the SVM for classification. Since it is a kernel method, it is based on distance. Therefore, we need to scale the data first."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H1imR_MAWvB3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "expdf = pd.concat([X[selected], y], axis = 1)\n",
        "scaler.fit(expdf)\n",
        "df_scaled = scaler.transform(expdf)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WTjuTjBJfvQ5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_scaled = df_scaled[:, :-1]\n",
        "y_scaled = df_scaled[:, -1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ARPZIs_IWvB5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Xsc_train, Xsc_test, ysc_train, ysc_test = train_test_split(X_scaled, y_scaled, test_size=0.3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S26lRgEuWvB7",
        "colab_type": "code",
        "outputId": "11927e03-7d9e-4507-f2eb-b77a4adf3ce6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "from sklearn.svm import SVR\n",
        "svr = SVR(kernel='rbf')\n",
        "svr.fit(Xsc_train, ysc_train)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SVR(C=1.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.1,\n",
              "    gamma='auto_deprecated', kernel='rbf', max_iter=-1, shrinking=True,\n",
              "    tol=0.001, verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5T_GztZjhXWq",
        "colab_type": "code",
        "outputId": "c7ec862b-80e8-4164-a458-8a1129faf1ec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(\"The coefficient of determination R^2 for train data is\", round(svr.score(Xsc_train, ysc_train), 4))\n",
        "print(\"The coefficient of determination R^2 for test data is\", round(svr.score(Xsc_test, ysc_test), 4))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The coefficient of determination R^2 for train data is 0.7894\n",
            "The coefficient of determination R^2 for test data is 0.7706\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RqcJ5WM3WvCA",
        "colab_type": "text"
      },
      "source": [
        "Now we can see a substantial increase in the $R^2$ coefficient compared to multiple linear regression - more than 8% for train data and about 7% for test data. This might be explained by the fact that this method used RBF kernel which can track non-linear trends (unlike linear regression). Also, there is no overfitting or underfitting since both coefficients are almost similar. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HlMR0EKvWvCA",
        "colab_type": "text"
      },
      "source": [
        "The next widespread but very powerful method is Random Forest. RF is a tree-based model and hence does not require feature scaling. It is quite hard to determine the best parameters for this method; that's why I will use GridSearchCV which search over specified parameters to find the best one."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e2FS25_mWvCB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "parameters = {'max_depth': list(range(1, 15)), 'min_samples_leaf': list(range(1,10))}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D4tVxiuEWvCC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rf = RandomForestRegressor()\n",
        "gcv = GridSearchCV(rf, parameters, n_jobs = -1, cv = 5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wbnLj0z_WvCE",
        "colab_type": "code",
        "outputId": "f74a4b81-b772-49a8-8b40-b406ad658fc9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        }
      },
      "source": [
        "gcv.fit(X_train, y_train)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
              "             estimator=RandomForestRegressor(bootstrap=True, criterion='mse',\n",
              "                                             max_depth=None,\n",
              "                                             max_features='auto',\n",
              "                                             max_leaf_nodes=None,\n",
              "                                             min_impurity_decrease=0.0,\n",
              "                                             min_impurity_split=None,\n",
              "                                             min_samples_leaf=1,\n",
              "                                             min_samples_split=2,\n",
              "                                             min_weight_fraction_leaf=0.0,\n",
              "                                             n_estimators=100, n_jobs=None,\n",
              "                                             oob_score=False, random_state=17,\n",
              "                                             verbose=0, warm_start=False),\n",
              "             iid='warn', n_jobs=-1,\n",
              "             param_grid={'max_depth': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,\n",
              "                                       13, 14],\n",
              "                         'min_samples_leaf': [1, 2, 3, 4, 5, 6, 7, 8, 9]},\n",
              "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
              "             scoring=None, verbose=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XxRgZyhN5Ej0",
        "colab_type": "text"
      },
      "source": [
        "As GridSearch determined, the best score can be reached by using max_depth = 11 and min_samples_leaf = 1:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "yrS-vt_DWvCF",
        "colab_type": "code",
        "outputId": "a8f12282-9f50-4e19-c717-0449855e4b82",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "gcv.best_params_"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'max_depth': 11, 'min_samples_leaf': 1}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ppMK_So1X1il",
        "colab_type": "code",
        "outputId": "68c33f27-42b7-428e-950d-278f84677e68",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "rfm = RandomForestRegressor(max_depth = 11, min_samples_leaf = 1)\n",
        "rfm.fit(X_train, y_train)\n",
        "print('The coefficient of determination R^2 for train data is', round(rfm.score(X_train, y_train), 4))\n",
        "print('The coefficient of determination R^2 for test data is', round(rfm.score(X_test, y_test), 4))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The coefficient of determination R^2 for train data is 0.841\n",
            "The coefficient of determination R^2 for test data is 0.761\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3eExDL-W5Y6p",
        "colab_type": "text"
      },
      "source": [
        "Although $R^2$ is higher than in linear regression case, RF regressor does not perform as good as SVR with rbf kernel. Also, we can observe some overfitting since the measure for train data is 8% larger than for test data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uHawQHxB6fDK",
        "colab_type": "text"
      },
      "source": [
        "It would also be worth trying another method which is called Gradient Boosting. GB and RF are quite similar to each other in many ways; however, they differ in the way the trees are built, and GB might often perform better."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1LjWk6Yi615w",
        "colab_type": "text"
      },
      "source": [
        "After parameters tuning, I obtained the following result:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_n_QdKmQWvCK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.ensemble import GradientBoostingRegressor "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UxPKGNvxWvCL",
        "colab_type": "code",
        "outputId": "26d59d99-1ba0-40d2-e72e-662ddcc42cc2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "gbrt=GradientBoostingRegressor(n_estimators = 100, max_depth = 7, min_samples_leaf = 2, max_features= 5) \n",
        "gbrt.fit(X_train, y_train)\n",
        "print('The coefficient of determination R^2 for train data is', round(gbrt.score(X_train, y_train), 4)) #836 & 775\n",
        "print('The coefficient of determination R^2 for test data is', round(gbrt.score(X_test, y_test), 4))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The coefficient of determination R^2 for train data is 0.831\n",
            "The coefficient of determination R^2 for test data is 0.779\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88tdFetO7DDy",
        "colab_type": "text"
      },
      "source": [
        "What we can see is that this model slightly outperforms SVR and therefore is better than all other models. However, the difference in $R^2$ is quite negligible.\n",
        "\n",
        "There is also one more method which has been developed recently. It takes into account categorical variables and therefore might explain trends better. Since feature selection method indicated that tourney_level variable (which is categorical) is important, we can try this type of Regressor:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8kCQNv0oWvCM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from catboost import CatBoostRegressor\n",
        "from catboost import Pool"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B5jM-MvVxBuN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "categorical_features_indices = np.array([0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1CWr0OmiWvCN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cbr = CatBoostRegressor(iterations = 97,\n",
        "                          depth = 7 ,\n",
        "                          use_best_model = True, learning_rate = 0.25)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mQFNBqEoWvCN",
        "colab_type": "code",
        "outputId": "ae5c679c-d4e4-4bed-f23c-6765e3b19d5c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "cbr.fit(X_train, y_train,\n",
        "          cat_features = categorical_features_indices, eval_set=(X_test, y_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0:\tlearn: 2.4407334\ttest: 2.4706671\tbest: 2.4706671 (0)\ttotal: 18.8ms\tremaining: 1.8s\n",
            "1:\tlearn: 2.2198887\ttest: 2.2401058\tbest: 2.2401058 (1)\ttotal: 32.9ms\tremaining: 1.56s\n",
            "2:\tlearn: 2.0504829\ttest: 2.0629966\tbest: 2.0629966 (2)\ttotal: 47ms\tremaining: 1.47s\n",
            "3:\tlearn: 1.8562715\ttest: 1.8628891\tbest: 1.8628891 (3)\ttotal: 61ms\tremaining: 1.42s\n",
            "4:\tlearn: 1.7266487\ttest: 1.7271567\tbest: 1.7271567 (4)\ttotal: 74.7ms\tremaining: 1.37s\n",
            "5:\tlearn: 1.6421248\ttest: 1.6401755\tbest: 1.6401755 (5)\ttotal: 88.3ms\tremaining: 1.34s\n",
            "6:\tlearn: 1.5761987\ttest: 1.5734226\tbest: 1.5734226 (6)\ttotal: 102ms\tremaining: 1.3s\n",
            "7:\tlearn: 1.5294367\ttest: 1.5259768\tbest: 1.5259768 (7)\ttotal: 116ms\tremaining: 1.29s\n",
            "8:\tlearn: 1.4904405\ttest: 1.4880089\tbest: 1.4880089 (8)\ttotal: 129ms\tremaining: 1.26s\n",
            "9:\tlearn: 1.4603958\ttest: 1.4570239\tbest: 1.4570239 (9)\ttotal: 142ms\tremaining: 1.23s\n",
            "10:\tlearn: 1.4382905\ttest: 1.4333891\tbest: 1.4333891 (10)\ttotal: 157ms\tremaining: 1.23s\n",
            "11:\tlearn: 1.4217308\ttest: 1.4175301\tbest: 1.4175301 (11)\ttotal: 170ms\tremaining: 1.2s\n",
            "12:\tlearn: 1.4074524\ttest: 1.4027416\tbest: 1.4027416 (12)\ttotal: 183ms\tremaining: 1.18s\n",
            "13:\tlearn: 1.3955940\ttest: 1.3909297\tbest: 1.3909297 (13)\ttotal: 197ms\tremaining: 1.17s\n",
            "14:\tlearn: 1.3880477\ttest: 1.3835231\tbest: 1.3835231 (14)\ttotal: 210ms\tremaining: 1.15s\n",
            "15:\tlearn: 1.3818545\ttest: 1.3772871\tbest: 1.3772871 (15)\ttotal: 226ms\tremaining: 1.14s\n",
            "16:\tlearn: 1.3771480\ttest: 1.3732272\tbest: 1.3732272 (16)\ttotal: 239ms\tremaining: 1.12s\n",
            "17:\tlearn: 1.3674706\ttest: 1.3648586\tbest: 1.3648586 (17)\ttotal: 253ms\tremaining: 1.11s\n",
            "18:\tlearn: 1.3633880\ttest: 1.3615308\tbest: 1.3615308 (18)\ttotal: 268ms\tremaining: 1.1s\n",
            "19:\tlearn: 1.3578648\ttest: 1.3564879\tbest: 1.3564879 (19)\ttotal: 283ms\tremaining: 1.09s\n",
            "20:\tlearn: 1.3533355\ttest: 1.3525760\tbest: 1.3525760 (20)\ttotal: 297ms\tremaining: 1.07s\n",
            "21:\tlearn: 1.3502160\ttest: 1.3505618\tbest: 1.3505618 (21)\ttotal: 313ms\tremaining: 1.07s\n",
            "22:\tlearn: 1.3450477\ttest: 1.3460753\tbest: 1.3460753 (22)\ttotal: 326ms\tremaining: 1.05s\n",
            "23:\tlearn: 1.3423258\ttest: 1.3434735\tbest: 1.3434735 (23)\ttotal: 339ms\tremaining: 1.03s\n",
            "24:\tlearn: 1.3392248\ttest: 1.3419799\tbest: 1.3419799 (24)\ttotal: 352ms\tremaining: 1.01s\n",
            "25:\tlearn: 1.3374359\ttest: 1.3408422\tbest: 1.3408422 (25)\ttotal: 366ms\tremaining: 998ms\n",
            "26:\tlearn: 1.3348737\ttest: 1.3397777\tbest: 1.3397777 (26)\ttotal: 380ms\tremaining: 985ms\n",
            "27:\tlearn: 1.3319191\ttest: 1.3375693\tbest: 1.3375693 (27)\ttotal: 393ms\tremaining: 970ms\n",
            "28:\tlearn: 1.3307158\ttest: 1.3366374\tbest: 1.3366374 (28)\ttotal: 406ms\tremaining: 953ms\n",
            "29:\tlearn: 1.3287581\ttest: 1.3356064\tbest: 1.3356064 (29)\ttotal: 419ms\tremaining: 936ms\n",
            "30:\tlearn: 1.3265295\ttest: 1.3339891\tbest: 1.3339891 (30)\ttotal: 439ms\tremaining: 934ms\n",
            "31:\tlearn: 1.3255728\ttest: 1.3330561\tbest: 1.3330561 (31)\ttotal: 453ms\tremaining: 919ms\n",
            "32:\tlearn: 1.3237858\ttest: 1.3318567\tbest: 1.3318567 (32)\ttotal: 466ms\tremaining: 904ms\n",
            "33:\tlearn: 1.3222202\ttest: 1.3305664\tbest: 1.3305664 (33)\ttotal: 479ms\tremaining: 888ms\n",
            "34:\tlearn: 1.3197950\ttest: 1.3291970\tbest: 1.3291970 (34)\ttotal: 493ms\tremaining: 873ms\n",
            "35:\tlearn: 1.3172712\ttest: 1.3276239\tbest: 1.3276239 (35)\ttotal: 506ms\tremaining: 858ms\n",
            "36:\tlearn: 1.3157380\ttest: 1.3273710\tbest: 1.3273710 (36)\ttotal: 520ms\tremaining: 843ms\n",
            "37:\tlearn: 1.3140781\ttest: 1.3266084\tbest: 1.3266084 (37)\ttotal: 534ms\tremaining: 829ms\n",
            "38:\tlearn: 1.3131824\ttest: 1.3261516\tbest: 1.3261516 (38)\ttotal: 547ms\tremaining: 813ms\n",
            "39:\tlearn: 1.3113952\ttest: 1.3247114\tbest: 1.3247114 (39)\ttotal: 561ms\tremaining: 799ms\n",
            "40:\tlearn: 1.3098930\ttest: 1.3239732\tbest: 1.3239732 (40)\ttotal: 573ms\tremaining: 783ms\n",
            "41:\tlearn: 1.3086281\ttest: 1.3230450\tbest: 1.3230450 (41)\ttotal: 587ms\tremaining: 769ms\n",
            "42:\tlearn: 1.3070578\ttest: 1.3229295\tbest: 1.3229295 (42)\ttotal: 600ms\tremaining: 754ms\n",
            "43:\tlearn: 1.3051466\ttest: 1.3225186\tbest: 1.3225186 (43)\ttotal: 614ms\tremaining: 739ms\n",
            "44:\tlearn: 1.3041308\ttest: 1.3216484\tbest: 1.3216484 (44)\ttotal: 626ms\tremaining: 724ms\n",
            "45:\tlearn: 1.3024305\ttest: 1.3207686\tbest: 1.3207686 (45)\ttotal: 647ms\tremaining: 717ms\n",
            "46:\tlearn: 1.3009661\ttest: 1.3197716\tbest: 1.3197716 (46)\ttotal: 659ms\tremaining: 701ms\n",
            "47:\tlearn: 1.2995400\ttest: 1.3185006\tbest: 1.3185006 (47)\ttotal: 673ms\tremaining: 687ms\n",
            "48:\tlearn: 1.2983362\ttest: 1.3178493\tbest: 1.3178493 (48)\ttotal: 687ms\tremaining: 673ms\n",
            "49:\tlearn: 1.2961261\ttest: 1.3174401\tbest: 1.3174401 (49)\ttotal: 700ms\tremaining: 658ms\n",
            "50:\tlearn: 1.2945414\ttest: 1.3171246\tbest: 1.3171246 (50)\ttotal: 712ms\tremaining: 642ms\n",
            "51:\tlearn: 1.2932343\ttest: 1.3163627\tbest: 1.3163627 (51)\ttotal: 725ms\tremaining: 627ms\n",
            "52:\tlearn: 1.2917808\ttest: 1.3161073\tbest: 1.3161073 (52)\ttotal: 738ms\tremaining: 612ms\n",
            "53:\tlearn: 1.2904178\ttest: 1.3155916\tbest: 1.3155916 (53)\ttotal: 751ms\tremaining: 598ms\n",
            "54:\tlearn: 1.2891629\ttest: 1.3152509\tbest: 1.3152509 (54)\ttotal: 764ms\tremaining: 584ms\n",
            "55:\tlearn: 1.2876260\ttest: 1.3141621\tbest: 1.3141621 (55)\ttotal: 778ms\tremaining: 569ms\n",
            "56:\tlearn: 1.2866464\ttest: 1.3137992\tbest: 1.3137992 (56)\ttotal: 791ms\tremaining: 555ms\n",
            "57:\tlearn: 1.2854725\ttest: 1.3137358\tbest: 1.3137358 (57)\ttotal: 805ms\tremaining: 541ms\n",
            "58:\tlearn: 1.2841431\ttest: 1.3133847\tbest: 1.3133847 (58)\ttotal: 817ms\tremaining: 526ms\n",
            "59:\tlearn: 1.2829807\ttest: 1.3133726\tbest: 1.3133726 (59)\ttotal: 829ms\tremaining: 511ms\n",
            "60:\tlearn: 1.2814037\ttest: 1.3128504\tbest: 1.3128504 (60)\ttotal: 844ms\tremaining: 498ms\n",
            "61:\tlearn: 1.2802210\ttest: 1.3124940\tbest: 1.3124940 (61)\ttotal: 861ms\tremaining: 486ms\n",
            "62:\tlearn: 1.2798571\ttest: 1.3125696\tbest: 1.3124940 (61)\ttotal: 874ms\tremaining: 472ms\n",
            "63:\tlearn: 1.2789297\ttest: 1.3123247\tbest: 1.3123247 (63)\ttotal: 887ms\tremaining: 458ms\n",
            "64:\tlearn: 1.2780734\ttest: 1.3121508\tbest: 1.3121508 (64)\ttotal: 900ms\tremaining: 443ms\n",
            "65:\tlearn: 1.2775544\ttest: 1.3118943\tbest: 1.3118943 (65)\ttotal: 913ms\tremaining: 429ms\n",
            "66:\tlearn: 1.2766226\ttest: 1.3117121\tbest: 1.3117121 (66)\ttotal: 926ms\tremaining: 415ms\n",
            "67:\tlearn: 1.2757440\ttest: 1.3115364\tbest: 1.3115364 (67)\ttotal: 939ms\tremaining: 400ms\n",
            "68:\tlearn: 1.2743892\ttest: 1.3108034\tbest: 1.3108034 (68)\ttotal: 952ms\tremaining: 386ms\n",
            "69:\tlearn: 1.2735917\ttest: 1.3104411\tbest: 1.3104411 (69)\ttotal: 975ms\tremaining: 376ms\n",
            "70:\tlearn: 1.2734853\ttest: 1.3103577\tbest: 1.3103577 (70)\ttotal: 987ms\tremaining: 361ms\n",
            "71:\tlearn: 1.2728594\ttest: 1.3103941\tbest: 1.3103577 (70)\ttotal: 1s\tremaining: 347ms\n",
            "72:\tlearn: 1.2724862\ttest: 1.3104179\tbest: 1.3103577 (70)\ttotal: 1.01s\tremaining: 333ms\n",
            "73:\tlearn: 1.2713834\ttest: 1.3100379\tbest: 1.3100379 (73)\ttotal: 1.02s\tremaining: 319ms\n",
            "74:\tlearn: 1.2701468\ttest: 1.3100531\tbest: 1.3100379 (73)\ttotal: 1.04s\tremaining: 305ms\n",
            "75:\tlearn: 1.2691794\ttest: 1.3098145\tbest: 1.3098145 (75)\ttotal: 1.06s\tremaining: 293ms\n",
            "76:\tlearn: 1.2684445\ttest: 1.3095654\tbest: 1.3095654 (76)\ttotal: 1.07s\tremaining: 278ms\n",
            "77:\tlearn: 1.2673519\ttest: 1.3098473\tbest: 1.3095654 (76)\ttotal: 1.08s\tremaining: 264ms\n",
            "78:\tlearn: 1.2662708\ttest: 1.3094976\tbest: 1.3094976 (78)\ttotal: 1.1s\tremaining: 250ms\n",
            "79:\tlearn: 1.2653693\ttest: 1.3093498\tbest: 1.3093498 (79)\ttotal: 1.11s\tremaining: 236ms\n",
            "80:\tlearn: 1.2642157\ttest: 1.3091941\tbest: 1.3091941 (80)\ttotal: 1.12s\tremaining: 222ms\n",
            "81:\tlearn: 1.2633376\ttest: 1.3088323\tbest: 1.3088323 (81)\ttotal: 1.14s\tremaining: 208ms\n",
            "82:\tlearn: 1.2627463\ttest: 1.3088475\tbest: 1.3088323 (81)\ttotal: 1.15s\tremaining: 194ms\n",
            "83:\tlearn: 1.2614279\ttest: 1.3086289\tbest: 1.3086289 (83)\ttotal: 1.16s\tremaining: 180ms\n",
            "84:\tlearn: 1.2605758\ttest: 1.3086976\tbest: 1.3086289 (83)\ttotal: 1.17s\tremaining: 166ms\n",
            "85:\tlearn: 1.2598357\ttest: 1.3087324\tbest: 1.3086289 (83)\ttotal: 1.19s\tremaining: 152ms\n",
            "86:\tlearn: 1.2588366\ttest: 1.3087019\tbest: 1.3086289 (83)\ttotal: 1.2s\tremaining: 138ms\n",
            "87:\tlearn: 1.2577890\ttest: 1.3083417\tbest: 1.3083417 (87)\ttotal: 1.21s\tremaining: 124ms\n",
            "88:\tlearn: 1.2570013\ttest: 1.3084686\tbest: 1.3083417 (87)\ttotal: 1.23s\tremaining: 110ms\n",
            "89:\tlearn: 1.2563201\ttest: 1.3085791\tbest: 1.3083417 (87)\ttotal: 1.25s\tremaining: 97.2ms\n",
            "90:\tlearn: 1.2552240\ttest: 1.3082868\tbest: 1.3082868 (90)\ttotal: 1.26s\tremaining: 83.3ms\n",
            "91:\tlearn: 1.2542861\ttest: 1.3078470\tbest: 1.3078470 (91)\ttotal: 1.28s\tremaining: 69.4ms\n",
            "92:\tlearn: 1.2535955\ttest: 1.3078818\tbest: 1.3078470 (91)\ttotal: 1.29s\tremaining: 55.5ms\n",
            "93:\tlearn: 1.2528160\ttest: 1.3076885\tbest: 1.3076885 (93)\ttotal: 1.3s\tremaining: 41.6ms\n",
            "94:\tlearn: 1.2523630\ttest: 1.3075051\tbest: 1.3075051 (94)\ttotal: 1.31s\tremaining: 27.7ms\n",
            "95:\tlearn: 1.2512815\ttest: 1.3074485\tbest: 1.3074485 (95)\ttotal: 1.33s\tremaining: 13.8ms\n",
            "96:\tlearn: 1.2504095\ttest: 1.3073302\tbest: 1.3073302 (96)\ttotal: 1.34s\tremaining: 0us\n",
            "\n",
            "bestTest = 1.30733016\n",
            "bestIteration = 96\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<catboost.core.CatBoostRegressor at 0x7f093c2758d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 257
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K3891X7foYgE",
        "colab_type": "code",
        "outputId": "49dc6f4c-b56e-4bef-cd10-d551247962aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(cbr.score(X_train, y_train))\n",
        "print(cbr.score(X_test, y_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.7938895335461807\n",
            "0.7816034214381226\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GUDzlrij8MG0",
        "colab_type": "text"
      },
      "source": [
        "Having performed 100 itirations, the algorithm detected that the highest $R^2$ could be achieved at the 97th itiration. Now it is above 78%, which is the highest value out of all previous methods.\n",
        "\n",
        "Finally, I check the performance of ANN. I chose MSE as a loss function as it is the default loss to use for regression problems. In the hidden layers, the activation function is ReLu - it is considered to be one of the best functions for neural nets. In the output layer with one node, I use linear activation function because the target variable is continuous:  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jg2YqSO_WvCO",
        "colab_type": "code",
        "outputId": "94be8314-7fa0-40c3-f9e0-e610eaa7eb07",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        }
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation\n",
        "from keras.optimizers import Adam, SGD\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.layers.core import Dropout"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lX2ZmnJ-x_Em",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epoc = 50 #@param {type:\"slider\", min:0, max:100, step:1}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qdjt-IBhWvCP",
        "colab_type": "code",
        "outputId": "8b402d7a-dabb-4c7e-dee8-d265194296aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(32, input_shape=(X_train.shape[1],), activation='relu', kernel_initializer='normal'))\n",
        "model.add(Dense(28,activation='relu'))\n",
        "model.add(Dense(1, activation='linear'))\n",
        "model.compile(optimizer = 'Adam', loss = 'mean_squared_error', metrics = ['mse'])\n",
        "model.fit(X_train, y_train, epochs=epoc, callbacks = [EarlyStopping(monitor='mse', patience=2)])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "31550/31550 [==============================] - 3s 84us/step - loss: 6.1464 - mean_squared_error: 6.1464\n",
            "Epoch 2/50\n",
            "31550/31550 [==============================] - 1s 41us/step - loss: 2.5123 - mean_squared_error: 2.5123\n",
            "Epoch 3/50\n",
            "31550/31550 [==============================] - 1s 43us/step - loss: 2.3786 - mean_squared_error: 2.3786\n",
            "Epoch 4/50\n",
            "31550/31550 [==============================] - 1s 44us/step - loss: 2.2819 - mean_squared_error: 2.2819\n",
            "Epoch 5/50\n",
            "31550/31550 [==============================] - 1s 41us/step - loss: 2.1612 - mean_squared_error: 2.1612\n",
            "Epoch 6/50\n",
            "31550/31550 [==============================] - 1s 44us/step - loss: 2.0832 - mean_squared_error: 2.0832\n",
            "Epoch 7/50\n",
            "31550/31550 [==============================] - 1s 43us/step - loss: 2.0131 - mean_squared_error: 2.0131\n",
            "Epoch 8/50\n",
            "31550/31550 [==============================] - 1s 44us/step - loss: 1.9750 - mean_squared_error: 1.9750\n",
            "Epoch 9/50\n",
            "31550/31550 [==============================] - 1s 43us/step - loss: 1.9372 - mean_squared_error: 1.9372\n",
            "Epoch 10/50\n",
            "31550/31550 [==============================] - 1s 42us/step - loss: 1.9321 - mean_squared_error: 1.9321\n",
            "Epoch 11/50\n",
            "31550/31550 [==============================] - 1s 41us/step - loss: 1.9184 - mean_squared_error: 1.9184\n",
            "Epoch 12/50\n",
            "31550/31550 [==============================] - 1s 42us/step - loss: 1.8943 - mean_squared_error: 1.8943\n",
            "Epoch 13/50\n",
            "31550/31550 [==============================] - 1s 41us/step - loss: 1.9056 - mean_squared_error: 1.9056\n",
            "Epoch 14/50\n",
            "31550/31550 [==============================] - 1s 44us/step - loss: 1.8799 - mean_squared_error: 1.8799\n",
            "Epoch 15/50\n",
            "31550/31550 [==============================] - 1s 43us/step - loss: 1.8855 - mean_squared_error: 1.8855\n",
            "Epoch 16/50\n",
            "31550/31550 [==============================] - 1s 43us/step - loss: 1.8713 - mean_squared_error: 1.8713\n",
            "Epoch 17/50\n",
            "31550/31550 [==============================] - 1s 41us/step - loss: 1.8646 - mean_squared_error: 1.8646\n",
            "Epoch 18/50\n",
            "31550/31550 [==============================] - 1s 41us/step - loss: 1.8643 - mean_squared_error: 1.8643\n",
            "Epoch 19/50\n",
            "31550/31550 [==============================] - 1s 41us/step - loss: 1.8571 - mean_squared_error: 1.8571\n",
            "Epoch 20/50\n",
            "31550/31550 [==============================] - 1s 43us/step - loss: 1.8381 - mean_squared_error: 1.8381\n",
            "Epoch 21/50\n",
            "31550/31550 [==============================] - 1s 44us/step - loss: 1.8328 - mean_squared_error: 1.8328\n",
            "Epoch 22/50\n",
            "31550/31550 [==============================] - 1s 42us/step - loss: 1.8304 - mean_squared_error: 1.8304\n",
            "Epoch 23/50\n",
            "31550/31550 [==============================] - 1s 41us/step - loss: 1.8232 - mean_squared_error: 1.8232\n",
            "Epoch 24/50\n",
            "31550/31550 [==============================] - 1s 40us/step - loss: 1.8272 - mean_squared_error: 1.8272\n",
            "Epoch 25/50\n",
            "31550/31550 [==============================] - 1s 41us/step - loss: 1.8136 - mean_squared_error: 1.8136\n",
            "Epoch 26/50\n",
            "31550/31550 [==============================] - 1s 42us/step - loss: 1.8250 - mean_squared_error: 1.8250\n",
            "Epoch 27/50\n",
            "31550/31550 [==============================] - 1s 43us/step - loss: 1.8194 - mean_squared_error: 1.8194\n",
            "Epoch 28/50\n",
            "31550/31550 [==============================] - 1s 42us/step - loss: 1.8075 - mean_squared_error: 1.8075\n",
            "Epoch 29/50\n",
            "31550/31550 [==============================] - 1s 41us/step - loss: 1.8051 - mean_squared_error: 1.8051\n",
            "Epoch 30/50\n",
            "31550/31550 [==============================] - 1s 43us/step - loss: 1.8000 - mean_squared_error: 1.8000\n",
            "Epoch 31/50\n",
            "31550/31550 [==============================] - 1s 46us/step - loss: 1.8012 - mean_squared_error: 1.8012\n",
            "Epoch 32/50\n",
            "31550/31550 [==============================] - 1s 44us/step - loss: 1.8006 - mean_squared_error: 1.8006\n",
            "Epoch 33/50\n",
            "31550/31550 [==============================] - 1s 44us/step - loss: 1.7945 - mean_squared_error: 1.7945\n",
            "Epoch 34/50\n",
            "31550/31550 [==============================] - 1s 43us/step - loss: 1.7914 - mean_squared_error: 1.7914\n",
            "Epoch 35/50\n",
            "31550/31550 [==============================] - 1s 42us/step - loss: 1.7948 - mean_squared_error: 1.7948\n",
            "Epoch 36/50\n",
            "31550/31550 [==============================] - 1s 42us/step - loss: 1.7851 - mean_squared_error: 1.7851\n",
            "Epoch 37/50\n",
            "31550/31550 [==============================] - 1s 43us/step - loss: 1.7861 - mean_squared_error: 1.7861\n",
            "Epoch 38/50\n",
            "31550/31550 [==============================] - 1s 42us/step - loss: 1.7845 - mean_squared_error: 1.7845\n",
            "Epoch 39/50\n",
            "31550/31550 [==============================] - 1s 43us/step - loss: 1.7877 - mean_squared_error: 1.7877\n",
            "Epoch 40/50\n",
            "31550/31550 [==============================] - 1s 42us/step - loss: 1.7813 - mean_squared_error: 1.7813\n",
            "Epoch 41/50\n",
            "31550/31550 [==============================] - 1s 44us/step - loss: 1.7777 - mean_squared_error: 1.7777\n",
            "Epoch 42/50\n",
            "31550/31550 [==============================] - 1s 40us/step - loss: 1.7751 - mean_squared_error: 1.7751\n",
            "Epoch 43/50\n",
            "31550/31550 [==============================] - 1s 43us/step - loss: 1.7862 - mean_squared_error: 1.7862\n",
            "Epoch 44/50\n",
            "31550/31550 [==============================] - 1s 43us/step - loss: 1.7774 - mean_squared_error: 1.7774\n",
            "Epoch 45/50\n",
            "31550/31550 [==============================] - 1s 43us/step - loss: 1.7753 - mean_squared_error: 1.7753\n",
            "Epoch 46/50\n",
            "31550/31550 [==============================] - 1s 42us/step - loss: 1.7720 - mean_squared_error: 1.7720\n",
            "Epoch 47/50\n",
            "31550/31550 [==============================] - 1s 42us/step - loss: 1.7783 - mean_squared_error: 1.7783\n",
            "Epoch 48/50\n",
            "31550/31550 [==============================] - 1s 44us/step - loss: 1.7691 - mean_squared_error: 1.7691\n",
            "Epoch 49/50\n",
            "31550/31550 [==============================] - 1s 43us/step - loss: 1.7672 - mean_squared_error: 1.7672\n",
            "Epoch 50/50\n",
            "31550/31550 [==============================] - 1s 42us/step - loss: 1.7702 - mean_squared_error: 1.7702\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f08eb3be588>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 321
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oYK-3YutWvCQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_train_pred = model.predict(X_train)\n",
        "y_test_pred = model.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CNuLZssttRJv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import r2_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ChfabSGpWvCR",
        "colab_type": "code",
        "outputId": "2c244aba-c22f-49ab-f611-da3f317352d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(\"The R2 score on the Train set is:\\t{:0.3f}\".format(r2_score(y_train, y_train_pred)))\n",
        "print(\"The R2 score on the Test set is:\\t{:0.3f}\".format(r2_score(y_test, y_test_pred)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The R2 score on the Train set is:\t0.773\n",
            "The R2 score on the Test set is:\t0.778\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_qyZIvt_VhY",
        "colab_type": "text"
      },
      "source": [
        "This model performs a bit worse than GB but better than others. Since we use linear activation function, the last layer will be a linear function of the first layer. So probably hidden layers are not as useful in this case, that's why we didn't observe significant improvement of the score."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cDr4C7t63pfT",
        "colab_type": "text"
      },
      "source": [
        "#### Classification Problem"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PrqjoL5qARqZ",
        "colab_type": "text"
      },
      "source": [
        "In this part, I will try to predict whether a particular player will win if he plays agains some other player. So, it's a binary classification problem. The features used for this task are similar to ones used in the regression analysis."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nt4Oeen0WvCW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "features = ['surface', 'tourney_level', \"p1_hand\", \"p2_hand\", \"minutes\", \"p1_rank\", \"p2_rank\",\n",
        "           'p1_Q', 'p1_WC', 'p1_LL', 'p2_Q', 'p2_WC', 'p2_LL', 'ht_diff', 'p1_sign_country',\n",
        "           'p2_sign_country', 'age_diff', 'bpsaved_diff', 'aces_diff', 'df_diff', '1stIn_diff', '1stWon_diff',\n",
        "           '2ndWon_diff', 'outcome']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LtlcGtxFWvCX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = atp_class[features].drop(['outcome'], axis = 1)\n",
        "y = atp_class['outcome']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ytzXAtDmWvCX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QUmh1aN4ArP1",
        "colab_type": "text"
      },
      "source": [
        "First of all, the most common, easiest and sometimes most efficient approach is logistic regression:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5JumYvzfPY81",
        "colab_type": "code",
        "outputId": "b88887bc-6c85-43cf-f894-ff6bb0174641",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "LogReg = LogisticRegression()\n",
        "LogReg.fit(X_train, y_train)\n",
        "print(\"The accuracy for train data is\", round(LogReg.score(X_train, y_train), 4))\n",
        "print(\"The accuracy for test data is\", round(LogReg.score(X_test, y_test), 4))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The accuracy for train data is 0.9346\n",
            "The accuracy for test data is 0.9366\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "citOXhdEBA5W",
        "colab_type": "text"
      },
      "source": [
        "Apparently, this methods works fine with our data, giving the accuracy of more than 93% for both train and test sets. To make sure that the model is correct, let's take a look at the classification report and confusion matrix:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lzV7mAtjsqwp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_test_preds = LogReg.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YJnpj5dYs17o",
        "colab_type": "code",
        "outputId": "3e50ec4b-20a6-46bd-d9f7-452c164cc248",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(y_test, y_test_preds))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.93      0.93      0.93      6295\n",
            "           1       0.94      0.94      0.94      7675\n",
            "\n",
            "    accuracy                           0.93     13970\n",
            "   macro avg       0.93      0.93      0.93     13970\n",
            "weighted avg       0.93      0.93      0.93     13970\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X2NTXMhuBU-q",
        "colab_type": "text"
      },
      "source": [
        "Here we can notice that both precision and recall measures are high for two classes. That is supported by high values of F1 score. Therefore, the classifier works well in both directions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mv-cogRxs-67",
        "colab_type": "code",
        "outputId": "b62cba72-b68b-4ed9-ebf3-81dae8b669c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "print(confusion_matrix(y_test, y_test_preds))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[5845  450]\n",
            " [ 464 7211]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IsrZq-CUBTiX",
        "colab_type": "text"
      },
      "source": [
        "The confusion matrix is a way of tabulating the number of misclassifications, i.e. the number of predicted classes which ended up in a wrong classification bin based on the true classes.\n",
        "\n",
        "The diagonal elements show the number of correct classifications for each class. The off-diagonal elements provides the misclassifications: 450 of the class 0 were misclassified as 1, 464 of the class 1 were misclassified as 0. In total, it makes 914 misclassifications out of roughly 14 thousand, which is obviously a good result.\n",
        "\n",
        "As a next step, I want to check whether it is possible to reduce the dimensions of the data since now there are quite a lot of explanatory variables which makes the model complicated. So, it is necessary to scale data first and then apply PCA and see if some of the components explain the vast majority of the variance:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AcKxy_o0WvCb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(X)\n",
        "X_scaled = scaler.transform(X)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CgumI4E7sF58",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.decomposition import PCA\n",
        "model_pca = PCA().fit(X_scaled)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUXuSEYWDGql",
        "colab_type": "text"
      },
      "source": [
        "Below is the graph showing how much of the variance each component explains:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tEjhCiwXt06-",
        "colab_type": "code",
        "outputId": "aed9a7c8-055e-4569-b255-f3404ee206e8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 456
        }
      },
      "source": [
        "sns.set()\n",
        "fig = plt.figure(figsize=(8, 8));\n",
        "PCA_components = [\"PCA \" + str(i) for i in np.arange(1,24)]\n",
        "plt.bar(PCA_components,model_pca.explained_variance_)\n",
        "plt.gcf().autofmt_xdate()\n",
        "plt.xticks(fontsize=9)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAe4AAAG3CAYAAABylrZpAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nbgAADEJxAwBgEIobAACDUNwAABjk/wMum+1Hz4UOGQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 576x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W2aozYthDQiv",
        "colab_type": "text"
      },
      "source": [
        "Many of the components are quite important and explain a huge part of the total variance. Therefore, it's not that easy to drop even a small number of them. Even if we stick to the first 10 components, the accuracy will drop significantly (by 10%):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e0lsRWWouEqn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pca = PCA(n_components= 10).fit(X_scaled)\n",
        "X_pca = pca.transform(X_scaled)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kTx3umXasQNx",
        "colab_type": "code",
        "outputId": "22a78102-33bf-4fbf-b459-9253f9e0be15",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "LogReg3 = LogisticRegression()\n",
        "LogReg3.fit(X_pca, y)\n",
        "print(\"The accuracy for train data is\", round(LogReg3.score(X_pca, y), 4))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The accuracy for train data is 0.8318\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_ZqV3RLD4T8",
        "colab_type": "text"
      },
      "source": [
        "In the regression problem, SVR method (which is analog of SVM) showed one of the best results. For the classification purposes, SVM might be efficient as well. RBF kernel outperformed all other kernels, so I include the output for RBF below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m7zooRmCQ3Cq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Xsc_train, Xsc_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KptgotpeQ55R",
        "colab_type": "code",
        "outputId": "8cf61a6b-66e8-4932-b1f2-ccc0f8d42440",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "from sklearn.svm import SVC\n",
        "svm = SVC(kernel = 'rbf', C = 0.5)\n",
        "svm.fit(Xsc_train, y_train)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SVC(C=0.5, cache_size=200, class_weight=None, coef0=0.0,\n",
              "    decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
              "    kernel='rbf', max_iter=-1, probability=False, random_state=None,\n",
              "    shrinking=True, tol=0.001, verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 200
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cQAAN5MNREzf",
        "colab_type": "code",
        "outputId": "4253a184-d51d-4150-87a0-2abf7ad28637",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(\"The accuracy for train data is\", round(svm.score(Xsc_train, y_train), 4))\n",
        "print(\"The accuracy for test data is\", round(svm.score(Xsc_test, y_test), 4))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The accuracy for train data is 0.9369\n",
            "The accuracy for test data is 0.9349\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EgGxMlzRFijt",
        "colab_type": "text"
      },
      "source": [
        "The method performed well but nevertheless did not outperform logistic regression. Nevertheless, the accuracy is close enough. \n",
        "\n",
        "The next method to try is ANN. Like in the previous part, ReLu activation function was used for hidden layers. However, since now we're facing binary classification problem, we use binary crossentropy as a loss function and sigmoid activation function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uG6LxfkYiKpG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epoc = 50 #@param {type:\"slider\", min:0, max:100, step:1}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ffzCdE40iPf6",
        "colab_type": "code",
        "outputId": "e628fb04-3450-449f-8bb0-73bb883469a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        }
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(64, input_shape=(X_train.shape[1],), activation='relu'))\n",
        "model.add(Dense(32,activation='relu'))\n",
        "model.add(Dense(8,activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile('Adam', 'binary_crossentropy', metrics = ['accuracy'])\n",
        "model.fit(X_train, y_train, epochs=epoc, callbacks = [EarlyStopping(monitor='acc', patience=2)])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "32595/32595 [==============================] - 3s 103us/step - loss: 0.3416 - acc: 0.8946\n",
            "Epoch 2/50\n",
            "32595/32595 [==============================] - 2s 54us/step - loss: 0.2028 - acc: 0.9232\n",
            "Epoch 3/50\n",
            "32595/32595 [==============================] - 2s 54us/step - loss: 0.1789 - acc: 0.9297\n",
            "Epoch 4/50\n",
            "32595/32595 [==============================] - 2s 55us/step - loss: 0.1750 - acc: 0.9298\n",
            "Epoch 5/50\n",
            "32595/32595 [==============================] - 2s 55us/step - loss: 0.1779 - acc: 0.9299\n",
            "Epoch 6/50\n",
            "32595/32595 [==============================] - 2s 55us/step - loss: 0.1697 - acc: 0.9309\n",
            "Epoch 7/50\n",
            "32595/32595 [==============================] - 2s 55us/step - loss: 0.1665 - acc: 0.9335\n",
            "Epoch 8/50\n",
            "32595/32595 [==============================] - 2s 54us/step - loss: 0.1675 - acc: 0.9322\n",
            "Epoch 9/50\n",
            "32595/32595 [==============================] - 2s 53us/step - loss: 0.1652 - acc: 0.9337\n",
            "Epoch 10/50\n",
            "32595/32595 [==============================] - 2s 54us/step - loss: 0.1651 - acc: 0.9328\n",
            "Epoch 11/50\n",
            "32595/32595 [==============================] - 2s 53us/step - loss: 0.1650 - acc: 0.9328\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f9d34a04f28>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 275
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U6RB_6NCiKkG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_train_pred = model.predict_classes(X_train)\n",
        "y_test_pred = model.predict_classes(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S4AJANTgj6bv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import mean_squared_error as mse, accuracy_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SKSnW2NmjvtJ",
        "colab_type": "code",
        "outputId": "453fd9c9-65f9-4fc5-a7c3-1d63a36176fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(\"The accuracy score on the train set is:\\t{:0.3f}\".format(accuracy_score(y_train, y_train_pred)))\n",
        "print(\"The accuracy score on the test set is:\\t{:0.3f}\".format(accuracy_score(y_test, y_test_pred)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The accuracy score on the train set is:\t0.938\n",
            "The accuracy score on the test set is:\t0.935\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QyBH1wQYGV9n",
        "colab_type": "text"
      },
      "source": [
        "For ANN, it took only 11 epochs until Early Stopping callback stopped the training as the accuracy started to decrease. Despite tuning of parameters (such as the number of neurons), the model did not show any increase in performance, and the accuracy remained at the same level as before, just above 93%. \n",
        "\n",
        "Random Forest Classifier performed worse than any other algorithm even after GridSearch was used:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1o7DX8eAjzNs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M1I9Xb5Ampvr",
        "colab_type": "code",
        "outputId": "bddf3729-ad08-457f-b055-a4fa151a81f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "parameters = {'max_depth': list(range(1, 15)), 'min_samples_leaf': list(range(1,10))}\n",
        "rfc = RandomForestClassifier()\n",
        "gcv = GridSearchCV(rfc, parameters, n_jobs = -1, cv = 5)\n",
        "gcv.fit(X_train, y_train)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
              "             estimator=RandomForestClassifier(bootstrap=True, class_weight=None,\n",
              "                                              criterion='gini', max_depth=None,\n",
              "                                              max_features='auto',\n",
              "                                              max_leaf_nodes=None,\n",
              "                                              min_impurity_decrease=0.0,\n",
              "                                              min_impurity_split=None,\n",
              "                                              min_samples_leaf=1,\n",
              "                                              min_samples_split=2,\n",
              "                                              min_weight_fraction_leaf=0.0,\n",
              "                                              n_estimators='warn', n_jobs=None,\n",
              "                                              oob_score=False,\n",
              "                                              random_state=None, verbose=0,\n",
              "                                              warm_start=False),\n",
              "             iid='warn', n_jobs=-1,\n",
              "             param_grid={'max_depth': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,\n",
              "                                       13, 14],\n",
              "                         'min_samples_leaf': [1, 2, 3, 4, 5, 6, 7, 8, 9]},\n",
              "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
              "             scoring=None, verbose=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 273
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b9799zRBmzlJ",
        "colab_type": "code",
        "outputId": "7e9b552b-4a7b-4cfe-cc6c-c7a9f96b33f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "gcv.best_params_"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'max_depth': 12, 'min_samples_leaf': 7}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 277
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1AI7j0aWnlD0",
        "colab_type": "code",
        "outputId": "a6370c07-b15a-48c8-8aa6-a1fc11d77d6b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print('The accuracy for train data is', round(gcv.score(X_train, y_train), 4))\n",
        "print('The accuracy for test data is', round(gcv.score(X_test, y_test), 4))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The accuracy for train data is 0.9508\n",
            "The accuracy for test data is 0.9271\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mKzKOVRvHTZT",
        "colab_type": "text"
      },
      "source": [
        "#### Conclusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y4ZuvwvqHZx0",
        "colab_type": "text"
      },
      "source": [
        "To sum up, the best result for the regression problem showed Cat Boost which took into account categorical variables and reached $R^2$ of 78.15%. \n",
        "\n",
        "Gradient Boosting, Neural Nets and SVR performed a bit worse but still satisfactory. Each of the algorithms got $R^2$ above 77%. On the contrary, multiple linear regression performed poorly (the score was lower by ~7-8%). \n",
        "\n",
        "The results for the classification problem are quite different. The accuracy of more than 93% was reached by Logistic Regression, SVM with rbf kernel, and ANN. However, out of these models, I would not want to choose ANN just because it's computationally expensive and does not show any advantage over trivial Logistic Regression. "
      ]
    }
  ]
}